{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIVE STREAM INTO CLIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from datetime import datetime\n",
    "\n",
    "# Load YOLOv8 model with tracking enabled\n",
    "model = YOLO(\"best(3).pt\")  # Use yolov8s.pt or larger versions if needed\n",
    "\n",
    "# Function to create a video writer\n",
    "def get_video_writer(filename, frame_size, fps=30):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    return cv2.VideoWriter(filename, fourcc, fps, frame_size)\n",
    "\n",
    "# Initialize video capture (0 for webcam or replace with RTSP/HTTP stream URL)\n",
    "cap = cv2.VideoCapture('export6.mp4')\n",
    "\n",
    "# Object tracking dictionary\n",
    "active_recordings = {}\n",
    "frame_size = (int(cap.get(3)), int(cap.get(4)))\n",
    "frames_per_second = int(cap.get(cv2.CAP_PROP_FPS)) or 30  # Default to 30 if unknown\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Run YOLOv8 inference with tracking\n",
    "    results = model.track(frame, persist=True)\n",
    "    \n",
    "    if results[0].boxes.id is not None:\n",
    "        track_ids = results[0].boxes.id.cpu().numpy()\n",
    "        detections = results[0].boxes.xyxy.cpu().numpy()\n",
    "        class_ids = results[0].boxes.cls.cpu().numpy()\n",
    "    else:\n",
    "        track_ids, detections, class_ids = [], [], []\n",
    "    \n",
    "    detected_objects = set()\n",
    "    for bbox, cls_id, track_id in zip(detections, class_ids, track_ids):\n",
    "        label = f\"{model.names[int(cls_id)]}_{int(track_id)}\"\n",
    "        detected_objects.add(label)\n",
    "        \n",
    "        # Draw bounding boxes\n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # Start new recording if this specific object is not already being recorded\n",
    "        if label not in active_recordings:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"clip_{label}_{timestamp}.mp4\"\n",
    "            active_recordings[label] = get_video_writer(filename, frame_size, frames_per_second)\n",
    "            print(f\"Started recording: {filename}\")\n",
    "    \n",
    "    # Write frames to respective video files\n",
    "    for label in list(active_recordings.keys()):\n",
    "        if label in detected_objects:\n",
    "            active_recordings[label].write(frame)\n",
    "        else:\n",
    "            print(f\"Clip saved for {label}\")\n",
    "            active_recordings[label].release()\n",
    "            del active_recordings[label]\n",
    "    \n",
    "    # Show frame\n",
    "    cv2.imshow(\"YOLOv8 Live Stream\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "for writer in active_recordings.values():\n",
    "    writer.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Instantiate the VehicleSpeedEstimator with lane and distance information\n",
    "speed_estimator = VehicleSpeedEstimator(names=model.names, line1=line1, line2=line2,\n",
    "                                         distance_between_lines=distance_between_lines, scale_factor=scale_factor)\n",
    "\n",
    "# Process the video frames\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run YOLOv8 inference with tracking\n",
    "    results = model.track(frame, persist=True)\n",
    "    \n",
    "    if results[0].boxes.id is not None:\n",
    "        track_ids = results[0].boxes.id.cpu().numpy()\n",
    "        detections = results[0].boxes.xyxy.cpu().numpy()\n",
    "        class_ids = results[0].boxes.cls.cpu().numpy()\n",
    "    else:\n",
    "        track_ids, detections, class_ids = [], [], []\n",
    "\n",
    "    detected_objects = set()\n",
    "    for bbox, cls_id, track_id in zip(detections, class_ids, track_ids):\n",
    "        label = f\"{model.names[int(cls_id)]}_{int(track_id)}\"\n",
    "        detected_objects.add(label)\n",
    "\n",
    "        # Draw bounding boxes\n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Pass the current frame and tracks to the VehicleSpeedEstimator\n",
    "    current_time = time.time()  # Or any other time mechanism you use\n",
    "    frame = speed_estimator.process_frame(frame, results, current_time)\n",
    "\n",
    "    # Display the processed frame\n",
    "    cv2.imshow(\"Live Output\", frame)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "    # Write the processed frame to the output video\n",
    "    video_writer.write(frame)\n",
    "    \n",
    "    # Increment frame counter\n",
    "    frame_counter += 1\n",
    "\n",
    "    # Break the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        print(\"Processing terminated by user.\")\n",
    "        break\n",
    "\n",
    "# Release video capture and writer resources\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Processing complete. Output saved to {output_video}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUNNING PIPELINE ON DIRECTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reads the frame / add folder trajectory later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pedestrian 1 traveled 0.71 meters at 0.07 m/s.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from pedestrianpsm.LaneInput import LaneInput  # Class to input lane regions\n",
    "from carpsm.PSMorg import SpeedEstimator2  # Updated SpeedEstimator2 class with time increment\n",
    "from carpsm.time_utils import increment_time, get_time_from_xml  # Import utility functions\n",
    "from volume.volume import VehicleDetectionWithTracks \n",
    "from pedestrianpsm import LaneChecker\n",
    "from pedestrianpsm.PSMorg import PEDESTRIANESTIMATOR\n",
    "from pedestrianfeatures.PedestrianOrganization import PedestrianSpeedEstimator\n",
    "from vehicle.PSMorg import SpeedEstimator2\n",
    "from vehicle.Speed_3 import SpeedExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the YOLO model for object detection\n",
    "model = YOLO(r'E:\\learn phython\\speed 2\\Speed-detection-of-vehicles\\best(3).pt')  # Replace with the path to your YOLO model\n",
    "names = model.model.names  # List of class names for the YOLO model\n",
    "\n",
    "# Initialize video capture\n",
    "video_path = r'E:\\learn phython\\fydp final\\export6.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "assert cap.isOpened(), f\"Error reading video file {video_path}\"\n",
    "\n",
    "# Get video properties (width, height, and FPS)\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, \n",
    "                                      cv2.CAP_PROP_FRAME_HEIGHT, \n",
    "                                      cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Video writer to save the processed video with lane detection\n",
    "output_video = \"lane_detection_output.avi\"\n",
    "video_writer = cv2.VideoWriter(output_video, \n",
    "                               cv2.VideoWriter_fourcc(*\"mp4v\"), \n",
    "                               fps, (w, h))\n",
    "frame_counter = 0\n",
    "skip_frames = 10  # Number of frames to skip after processing one frame\n",
    "# Initialize LaneInput to capture lane points\n",
    "\n",
    "# Capture the first frame from the video to input lane points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LANE INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input the number of lanes:\n",
      "No lanes inputted. Using default lane regions.\n",
      "Default Lane Regions: [((509, 2), (622, 832), (988, 832), (731, 3)), ((733, 4), (987, 832), (1342, 833), (969, 5)), ((969, 5), (1344, 832), (1528, 832), (1233, 2))]\n",
      "Lane Centers: [(712, 417), (1007, 418), (1268, 417)]\n",
      "Captured Lane Regions: [((509, 2), (622, 832), (988, 832), (731, 3)), ((733, 4), (987, 832), (1342, 833), (969, 5)), ((969, 5), (1344, 832), (1528, 832), (1233, 2))] lane Centers: [(712, 417), (1007, 418), (1268, 417)]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SpeedEstimator2.__init__() missing 1 required positional argument: 'lane_regions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 38\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Frame processing\u001b[39;00m\n\u001b[0;32m     34\u001b[0m pedestrian_speed \u001b[38;5;241m=\u001b[39m PedestrianSpeedEstimator (\n\u001b[0;32m     35\u001b[0m     names\u001b[38;5;241m=\u001b[39m names\n\u001b[0;32m     36\u001b[0m )\n\u001b[1;32m---> 38\u001b[0m Car_Speed \u001b[38;5;241m=\u001b[39m \u001b[43mSpeedEstimator2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcurrent_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcurrent_time\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mview_img\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Display image flag   # Define the X-coordinate target for lane estimation\u001b[39;49;00m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlane_log.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Define the CSV file for logging\u001b[39;49;00m\n\u001b[0;32m     43\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Main loop for processing each video frame\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: SpeedEstimator2.__init__() missing 1 required positional argument: 'lane_regions'"
     ]
    }
   ],
   "source": [
    "lane_input = LaneInput()\n",
    "\n",
    "success, first_frame = cap.read()\n",
    "if not success:\n",
    "    raise ValueError(\"Failed to read the first frame from the video.\")\n",
    "\n",
    "# Input the lane regions in the first frame\n",
    "lane_input.input_lanes(first_frame)\n",
    "lane_regions , lane_centers = lane_input.get_lanes_and_centers()  # Get the captured lane regions\n",
    "print(\"Captured Lane Regions:\", lane_regions ,\"lane Centers:\" , lane_centers)\n",
    "\n",
    "# Load initial time from the XML file\n",
    "xml_file = r\"E:\\learn phython\\speed 2\\Speed-detection-of-vehicles\\C0148M01.XML\"\n",
    "current_time = get_time_from_xml(xml_file)\n",
    "\n",
    "# Initialize SpeedEstimator2 for VehICle PSM\n",
    "# lane_estimator = SpeedEstimator2(\n",
    "   # names=names,\n",
    "   # lane_regions=lane_regions,\n",
    "   # view_img=True,  # Display image flag\n",
    "   # target_x=500,   # Define the X-coordinate target for lane estimation\n",
    "   #  csv_file=\"lane_log.csv\" # Define the CSV file for logging)\n",
    "\n",
    "# Initialize SpeedEstimator2 for Pedestrian PSM\n",
    "lane_estimator = PEDESTRIANESTIMATOR(\n",
    "    names=names,\n",
    "    lane_regions=lane_regions,\n",
    "    lane_centers= lane_centers,\n",
    "    view_img=True,  # Display image flag   # Define the X-coordinate target for lane estimation\n",
    "    csv_file=\"lane_log.csv\"  # Define the CSV file for logging\n",
    ")\n",
    "# Frame processing\n",
    "\n",
    "pedestrian_speed = PedestrianSpeedEstimator (\n",
    "    names= names\n",
    ")\n",
    "\n",
    "Car_Speed = SpeedEstimator2(\n",
    "    names=names,\n",
    "    lane_regions=lane_regions\n",
    "    current_time = current_time ,\n",
    "    view_img=True,  # Display image flag   # Define the X-coordinate target for lane estimation\n",
    "    csv_file=\"lane_log.csv\"  # Define the CSV file for logging\n",
    ")\n",
    "# Main loop for processing each video frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()  # Read the next frame\n",
    "    if not success:\n",
    "        print(\"Video processing completed or frame not available.\")\n",
    "        break\n",
    "\n",
    "    # Process every (skip_frames + 1)-th frame\n",
    "    if frame_counter % (skip_frames + 1) == 0:\n",
    "        # Perform object detection and tracking using the YOLO model\n",
    "        tracks = model.track(im0, persist=True)\n",
    "        \n",
    "        # Increment the current time based on the number of frames skipped\n",
    "        current_time = increment_time(current_time, skip_frames + 1, fps)\n",
    "        \n",
    "        #CAR PSM CODE THAT RESULTS IN EXCEL FILES\n",
    "        # Pass the tracks and the frame to SpeedEstimator2's check_and_log_lanes function\n",
    "        #im0 = lane_estimator.check_and_log_lanes(im0, tracks, current_time)\n",
    "        \n",
    "        #PEDESTRIAN PSM CODE\n",
    "        #im0 = lane_estimator.process_frame(im0, tracks, current_time)\n",
    "        # Display the processed frame with lane estimation\n",
    "       \n",
    "       #PEDESTRIAN FEATURES\n",
    "       \n",
    "\n",
    "       #Vehicle Features\n",
    "        im0 = Car_Speed.Speed(im0 , tracks , current_time)\n",
    "        #FLOW\n",
    "       # = VehicleDetectionWithTracks(tracks, start_time=\"2025-02-27 08:00:00\", frame_rate=30)\n",
    "\n",
    "# Calculate the traffic flow based on the provided tracks\n",
    "        #vehicles_per_hour, interval_vehicles, max_flow_interval, max_flow_count = vehicle_detector.calculate_traffic_flow()\n",
    "\n",
    "# Output the results\n",
    "    #print(\"Total vehicles per hour:\")\n",
    "    #for hour, count in vehicles_per_hour.items():\n",
    "     #print(f\"{hour.strftime('%Y-%m-%d %H:%M:%S')} - {count} vehicles\")\n",
    "\n",
    "        # Display the processed frame with lane estimation\n",
    "    cv2.imshow(\"Live Output\", im0)\n",
    "    cv2.waitKey(1)\n",
    "        # Write the processed frame to the output video\n",
    "    video_writer.write(im0)\n",
    "\n",
    "    # Increment frame counter\n",
    "    frame_counter += 1\n",
    "\n",
    "    # Break the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        print(\"Processing terminated by user.\")\n",
    "        break\n",
    "\n",
    "# Release video capture and writer resources\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Processing complete. Output saved to {output_video}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VEHICLE FEATURES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pedestrian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
